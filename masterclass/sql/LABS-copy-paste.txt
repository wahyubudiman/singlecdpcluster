=================================================================================
  Lab 1: Handover cluster
=================================================================================

Goal: Get access to the system


1. Select a machine from the "List of machines" below
Add your name(s)
The IP address is <<yourhost>>

2. Ambari: http://<<yourhost>>:8080 
   
   Note: Please use Firefox or Chrome.

    - User: admin
    - Pass: 

3. Check dashboard. At least the following components should run and have a green status:

    - HDFS
    - MapReduce2
    - YARN
    - Tez
    - Hive
    - HBase
    - Zookeeper
    - Spark

4. Check that File View, Tez view and Hive view are available

5. Restart Hive server (else Lab 5 wil fail!)

6. Console via SSH or Web

    - User: student
    - Pass: 
    - If you do not have an SSH client, there is a web console: http://<<yourhost>>:4200



List of machines





=================================================================================
  Lab 2: The Ambari Views
=================================================================================

Goal: Use of Hive View and File View to issue SQL statements and understand link between files and tables

- Use File view to navigate to e.g. `/masterclass/lab2/customers`
- Use Hive view to query a table


SELECT * 
FROM customers 
WHERE length(last_name) > 7
ORDER BY last_name;



- Save the query

- Use Hive view to create a table and ...


CREATE EXTERNAL TABLE IF NOT EXISTS cust2 (
  last_name STRING,
  first_name STRING,
  house STRING,
  street STRING,
  post_code STRING
)
LOCATION "/tmp/cust2";


- ... and fill it


INSERT into cust2
SELECT last_name, first_name, address[0], address[1], address[3] 
FROM customers 
ORDER BY last_name;


- Refresh the Database Explorer and examine `cust2`.

- Look at the visual explanation of the last SQL statement

- Again, use File View to navigate to `/tmp/cust2`



=================================================================================
  Lab 3: Examine a data set and load it into hive
=================================================================================


Goal: Convert raw data to a table and then to an ORC table including some Hive SQL functions

- Use File View to navigate to `/masterclass/lab3/tweets`
- Use Hive View to create Schema (with data)


CREATE EXTERNAL TABLE IF NOT EXISTS tweets_text_partition(
  tweet_id bigint,
  created_unixtime bigint,
  created_time string,
  displayname string,
  msg string,
  fulltext string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY "|"
LOCATION "/masterclass/lab3/tweets";



- Use Database Explorer to look at schema and data 

- Prepare the data as ORC


CREATE EXTERNAL TABLE IF NOT EXISTS tweets_orc_msg_hashtags(
    tweet_id bigint,
    created_unixtime string,
    displayname string,
    msg string,
    hashtag string
)
STORED AS orc;



INSERT OVERWRITE TABLE tweets_orc_msg_hashtags
SELECT
  tweet_id,
  from_unixtime(floor(created_unixtime/1000)),
  displayname,
  msg,
  get_json_object(fulltext,'$.entities.hashtags[0].text')
FROM tweets_text_partition;



- Create a simple query

SELECT hashtag, count(*) as cnt
FROM tweets_orc_msg_hashtags
where hashtag is not null 
group by hashtag
having cnt>10
LIMIT 10;



=================================================================================
  Lab 4: Building a table on top of json
=================================================================================

Goal: Understand the Serialization/Deserialization feature

- Download the file `sample_twitter_data.txt` and examine the structure

- Create a schema for it ...


CREATE TABLE IF NOT EXISTS twitter_json (
  geolocation STRUCT<lat: DOUBLE, long: DOUBLE>,
  tweetmessage STRING,
  createddate STRING,
  `user` STRUCT<screenname: STRING,
                name: STRING,
                id: STRING,
                geoenabled: BOOLEAN,
                userlocation: STRING>
) 
ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe';

- ... and load data


LOAD DATA INPATH '/masterclass/lab4/twitter/sample_twitter_data.txt' 
INTO TABLE twitter_json;


- Query the file


SELECT * 
FROM twitter_json;


SELECT createddate, `user`.screenname 
FROM twitter_json WHERE `user`.name LIKE 'Sarah%';


Note: If you cannot add a SerDe jar to the overall Hive Path, then prefix the three commands above with

  ADD JAR /var/lib/hive/json-serde-1.1.9.9-Hive13-jar-with-dependencies.jar;





=================================================================================
  Lab 5: Use HBase table from Hive
=================================================================================

Goal: Understand external data sources using HBase as an example


1) Query HBase using Phoenix and Hive
Opoen a terminal (ssh) and call `phoenix-sqlline localhost:2181:/hbase-unsecure`

- Create view in Phoenix on HBase table:


CREATE VIEW "employees" ( pk VARCHAR PRIMARY KEY, "f"."birth_date" VARCHAR, "f"."first_name" VARCHAR, "f"."last_name" VARCHAR, "f"."gender" VARCHAR, "f"."hire_date" VARCHAR );


- Query from Phoenix :

SELECT * from "employees" limit 10;


- Create HBase view (hive table) from within Hive in Hive View ...


CREATE EXTERNAL TABLE employees_hbase(
  key BIGINT, 
  birth_date STRING, 
  first_name STRING, 
  last_name STRING, 
  gender STRING, 
  hire_date STRING
)
STORED BY "org.apache.hadoop.hive.hbase.HBaseStorageHandler"
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f:birth_date,f:first_name,f:last_name,f:gender,f:hire_date")
TBLPROPERTIES("hbase.table.name" = "employees");


- ... and query the new external HBase table in Hive


SELECT * from employees_hbase limit 10;




2) Create HBase table in Hive

- Use Hive view to create a table in HBase


CREATE TABLE tweets_hbase(
  tweet_id BIGINT, 
  created_unixtime BIGINT, 
  created_time STRING, 
  displayname STRING, 
  msg STRING, 
  fulltext STRING  
)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f:c1,f:c2,f:c3,f:c4,f:c5")
TBLPROPERTIES ("hbase.table.name" = "tweets");


- Goto Ambari Dashboard - HBase - Quick Links - HBase Master UI to check whether tweets table exists

- Insert data into HBase table


INSERT INTO tweets_hbase
SELECT * from tweets_text_partition;

- Query from Hive


SELECT * from tweets_hbase;




=================================================================================
  Lab 6: SQL explain 
=================================================================================


Goal: Understand the SQL explain

- Use Hive view to execute a join SQL statement and press _"Explain"_ instead of _"Execute_"


SELECT d.dept_name, count(*) as cnt
FROM departments d, employees e, dept_emp x
WHERE d.dept_no = x.dept_no and e.emp_no = x.emp_no
GROUP BY d.dept_name ORDER BY cnt DESC limit 5;  



ANALYZE TABLE departments COMPUTE STATISTICS for COLUMNS;
ANALYZE TABLE dept_emp COMPUTE STATISTICS for COLUMNS;
ANALYZE TABLE employees COMPUTE STATISTICS for COLUMNS;

ANALYZE TABLE departments COMPUTE STATISTICS;
ANALYZE TABLE dept_emp COMPUTE STATISTICS;
ANALYZE TABLE employees COMPUTE STATISTICS;

